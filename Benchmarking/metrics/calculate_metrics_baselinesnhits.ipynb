{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7f3ad9",
   "metadata": {},
   "source": [
    "# Calculate Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311ba3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bd1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to '/n/groups/patel/shakson/aiready/'\n",
    "import os\n",
    "os.chdir(\"/home/shaksonisaac/CGM/mambatf/\")\n",
    "\n",
    "#LOAD Datasets\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.cloud import storage\n",
    "\n",
    "_BUCKET_NAME = \"cgmproject2025\"\n",
    "\n",
    "# Download dataset from GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(_BUCKET_NAME)\n",
    "blob = bucket.blob('ai-ready/data/train_finaltimeseries_meal.feather')\n",
    "data_bytes = blob.download_as_bytes()\n",
    "train = pd.read_feather(io.BytesIO(data_bytes))\n",
    "\n",
    "\n",
    "# Download test set:\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(_BUCKET_NAME)\n",
    "blob = bucket.blob('ai-ready/data/test_finaltimeseries_meal.feather')\n",
    "data_bytes = blob.download_as_bytes()\n",
    "test = pd.read_feather(io.BytesIO(data_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e902f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-8)\n",
    "    )\n",
    "\n",
    "def quantile_loss(y_true, y_pred, q=0.5):\n",
    "    return np.mean(np.maximum(q * (y_true - y_pred), (q - 1) * (y_true - y_pred)))\n",
    "\n",
    "def calculate_metrics(t_grouped, train_df):\n",
    "    static_vars = train_df[\n",
    "        [\n",
    "            \"participant_id\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "    t_grouped = t_grouped.merge(static_vars, on=\"participant_id\", how=\"left\")\n",
    "    metrics_df = (\n",
    "        t_grouped.groupby(\"participant_id\")\n",
    "        .apply(\n",
    "            lambda df: pd.Series(\n",
    "                {\n",
    "                    \"SMAPE\": smape(df[\"target\"], df[\"prediction\"]),\n",
    "                    \"Quantile_Loss\": quantile_loss(df[\"target\"], df[\"prediction\"]),\n",
    "                    \"MAE\": np.mean(np.abs(df[\"prediction\"] - df[\"target\"])),\n",
    "                    \"RMSE\": np.sqrt(np.mean(np.square(df[\"prediction\"] - df[\"target\"]))),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    return metrics_df\n",
    "\n",
    "def get_confidence_intervals(df, metric, confidence=0.95):\n",
    "    mean = df[metric].mean()\n",
    "    sem = stats.sem(df[metric])  # Standard Error of the Mean\n",
    "    margin_of_error = sem * stats.t.ppf((1 + confidence) / 2, len(df) - 1)\n",
    "    return mean, mean - margin_of_error, mean + margin_of_error\n",
    "\n",
    "def calculate_metrics_CI(records, train):\n",
    "    \"\"\"\n",
    "    Calculate metrics for each participant in the records DataFrame.\n",
    "    \"\"\"\n",
    "    metrics_df = calculate_metrics(records, train)\n",
    "\n",
    "    confidence_intervals = {}\n",
    "    for metric in [\"SMAPE\", \"MAE\", \"RMSE\", \"Quantile_Loss\"]:\n",
    "        mean, lower, upper = get_confidence_intervals(metrics_df, metric)\n",
    "        confidence_intervals[metric] = {\n",
    "            \"mean\": mean,\n",
    "            \"lower\": lower,\n",
    "            \"upper\": upper\n",
    "        }\n",
    "    # Convert to DataFrame for better visualization\n",
    "    confidence_df = pd.DataFrame(confidence_intervals).T.reset_index()\n",
    "    confidence_df.columns = [\"Metric\", \"Mean\", \"Lower CI\", \"Upper CI\"]\n",
    "    return confidence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b10ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3041d",
   "metadata": {},
   "source": [
    "## Checkpointing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from google.cloud import storage\n",
    "\n",
    "def download_best_ckpt_by_filename(local_dir: str, bucket_name: str, gcs_prefix: str,\n",
    "                                   run_prefix: str = \"tft576-\", metric_key: str = \"val_loss\"):\n",
    "    \"\"\"\n",
    "    Finds the .ckpt with the smallest {metric_key} in its filename under gcs_prefix,\n",
    "    e.g., 'tft576-epoch=17-val_loss=3.61.ckpt', downloads it, and returns the local path.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    blobs = [b for b in bucket.list_blobs(prefix=gcs_prefix)\n",
    "             if b.name.endswith(\".ckpt\")\n",
    "             and \"last.ckpt\" not in b.name\n",
    "             and run_prefix in os.path.basename(b.name)\n",
    "             and f\"{metric_key}=\" in os.path.basename(b.name)]\n",
    "\n",
    "    if not blobs:\n",
    "        raise FileNotFoundError(f\"No epoch checkpoints with {metric_key}=... under gs://{bucket_name}/{gcs_prefix}\")\n",
    "\n",
    "    rx = re.compile(rf\"{metric_key}=([0-9]+\\.[0-9]+)\")\n",
    "    def score(b):\n",
    "        m = rx.search(os.path.basename(b.name))\n",
    "        return float(m.group(1)) if m else float(\"inf\")\n",
    "\n",
    "    best_blob = min(blobs, key=score)\n",
    "    local_path = os.path.join(local_dir, os.path.basename(best_blob.name))\n",
    "    best_blob.download_to_filename(local_path)\n",
    "    print(f\"Downloaded best checkpoint: gs://{bucket_name}/{best_blob.name} -> {local_path}\")\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b00fcd",
   "metadata": {},
   "source": [
    "# NHITS with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac64bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded best checkpoint: gs://cgmproject2025/checkpoints_nhits_288v4/nhits_288-epoch=04-val_loss=7.92.ckpt -> checkpoints/nhits_288-epoch=04-val_loss=7.92.ckpt\n",
      "[2025-09-02 23:11:52.216827] ðŸš€ Start of Dataloader Creation\n",
      "GPU Mem allocated: 0.00 GB | reserved: 0.01 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NHiTS(\n",
       "  \t\"activation\":                        ReLU\n",
       "  \t\"backcast_loss_ratio\":               0.0\n",
       "  \t\"batch_normalization\":               False\n",
       "  \t\"categorical_groups\":                {}\n",
       "  \t\"context_length\":                    288\n",
       "  \t\"dataset_parameters\":                {'time_idx': 'ds', 'target': 'cgm_glucose', 'group_ids': ['participant_id'], 'weight': None, 'max_encoder_length': 288, 'min_encoder_length': 288, 'min_prediction_idx': np.int64(11), 'min_prediction_length': 12, 'max_prediction_length': 12, 'static_categoricals': ['participant_id', 'clinical_site', 'study_group'], 'static_reals': ['age'], 'time_varying_known_categoricals': ['sleep_stage'], 'time_varying_known_reals': ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag'], 'time_varying_unknown_categoricals': None, 'time_varying_unknown_reals': ['cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std'], 'variable_groups': None, 'constant_fill_strategy': None, 'allow_missing_timesteps': False, 'lags': None, 'add_relative_time_idx': False, 'add_target_scales': True, 'add_encoder_length': True, 'target_normalizer': GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t), 'categorical_encoders': {'__group_id__participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'clinical_site': NaNLabelEncoder(add_nan=False, warn=True), 'study_group': NaNLabelEncoder(add_nan=False, warn=True), 'sleep_stage': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'age': StandardScaler(), 'encoder_length': StandardScaler(), 'cgm_glucose_center': StandardScaler(), 'cgm_glucose_scale': StandardScaler(), 'ds': StandardScaler(), 'minute_of_day': StandardScaler(), 'tod_sin': StandardScaler(), 'tod_cos': StandardScaler(), 'activity_steps': StandardScaler(), 'calories_value': StandardScaler(), 'heartrate': StandardScaler(), 'oxygen_saturation': StandardScaler(), 'respiration_rate': StandardScaler(), 'stress_level': StandardScaler(), 'predmeal_flag': StandardScaler(), 'cgm_lag_1': StandardScaler(), 'cgm_lag_3': StandardScaler(), 'cgm_lag_6': StandardScaler(), 'cgm_diff_lag_1': StandardScaler(), 'cgm_diff_lag_3': StandardScaler(), 'cgm_diff_lag_6': StandardScaler(), 'cgm_lagdiff_1_3': StandardScaler(), 'cgm_lagdiff_3_6': StandardScaler(), 'cgm_rolling_mean': StandardScaler(), 'cgm_rolling_std': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
       "  \t\"downsample_frequencies\":            [2]\n",
       "  \t\"dropout\":                           0.1\n",
       "  \t\"embedding_labels\":                  {'participant_id': {'1023': 0, '1024': 1, '1026': 2, '1027': 3, '1028': 4, '1029': 5, '1030': 6, '1031': 7, '1032': 8, '1033': 9, '1034': 10, '1035': 11, '1036': 12, '1037': 13, '1038': 14, '1039': 15, '1040': 16, '1041': 17, '1042': 18, '1044': 19, '1045': 20, '1046': 21, '1047': 22, '1049': 23, '1050': 24, '1051': 25, '1052': 26, '1053': 27, '1054': 28, '1055': 29, '1056': 30, '1057': 31, '1058': 32, '1060': 33, '1061': 34, '1062': 35, '1063': 36, '1064': 37, '1065': 38, '1066': 39, '1067': 40, '1068': 41, '1069': 42, '1070': 43, '1071': 44, '1072': 45, '1073': 46, '1074': 47, '1075': 48, '1076': 49, '1077': 50, '1079': 51, '1080': 52, '1081': 53, '1083': 54, '1084': 55, '1085': 56, '1086': 57, '1087': 58, '1088': 59, '1089': 60, '1092': 61, '1093': 62, '1094': 63, '1095': 64, '1096': 65, '1097': 66, '1098': 67, '1099': 68, '1100': 69, '1101': 70, '1103': 71, '1104': 72, '1105': 73, '1106': 74, '1109': 75, '1110': 76, '1112': 77, '1114': 78, '1115': 79, '1116': 80, '1117': 81, '1118': 82, '1119': 83, '1120': 84, '1121': 85, '1122': 86, '1124': 87, '1125': 88, '1126': 89, '1128': 90, '1129': 91, '1131': 92, '1132': 93, '1133': 94, '1134': 95, '1135': 96, '1136': 97, '1137': 98, '1138': 99, '1139': 100, '1140': 101, '1141': 102, '1143': 103, '1144': 104, '1145': 105, '1146': 106, '1148': 107, '1149': 108, '1151': 109, '1152': 110, '1153': 111, '1154': 112, '1155': 113, '1156': 114, '1157': 115, '1158': 116, '1159': 117, '1160': 118, '1161': 119, '1163': 120, '1164': 121, '1166': 122, '1167': 123, '1168': 124, '1169': 125, '1170': 126, '1171': 127, '1172': 128, '1173': 129, '1174': 130, '1175': 131, '1176': 132, '1177': 133, '1178': 134, '1179': 135, '1180': 136, '1181': 137, '1183': 138, '1184': 139, '1185': 140, '1186': 141, '1188': 142, '1189': 143, '1193': 144, '1194': 145, '1195': 146, '1196': 147, '1198': 148, '1199': 149, '1200': 150, '1201': 151, '1202': 152, '1203': 153, '1204': 154, '1205': 155, '1206': 156, '1207': 157, '1208': 158, '1209': 159, '1210': 160, '1211': 161, '1212': 162, '1213': 163, '1214': 164, '1215': 165, '1216': 166, '1217': 167, '1218': 168, '1219': 169, '1220': 170, '1221': 171, '1222': 172, '1223': 173, '1224': 174, '1225': 175, '1226': 176, '1227': 177, '1228': 178, '1229': 179, '1230': 180, '1231': 181, '1232': 182, '1233': 183, '1234': 184, '1235': 185, '1236': 186, '1237': 187, '1238': 188, '1239': 189, '1241': 190, '1242': 191, '1243': 192, '1244': 193, '1245': 194, '1246': 195, '1247': 196, '1248': 197, '1249': 198, '1250': 199, '1251': 200, '1252': 201, '1253': 202, '1254': 203, '1255': 204, '1256': 205, '1257': 206, '1258': 207, '1259': 208, '1260': 209, '1261': 210, '1262': 211, '1263': 212, '1264': 213, '1266': 214, '1267': 215, '1268': 216, '1269': 217, '1270': 218, '1271': 219, '1272': 220, '1273': 221, '1274': 222, '1275': 223, '1276': 224, '1277': 225, '1278': 226, '1280': 227, '1281': 228, '1283': 229, '1284': 230, '1285': 231, '1286': 232, '1287': 233, '1289': 234, '1290': 235, '1291': 236, '1292': 237, '1293': 238, '1294': 239, '1295': 240, '1297': 241, '1298': 242, '1300': 243, '1302': 244, '1303': 245, '1304': 246, '1305': 247, '1306': 248, '1307': 249, '1308': 250, '1309': 251, '1310': 252, '1311': 253, '1312': 254, '1313': 255, '1314': 256, '1315': 257, '1316': 258, '1317': 259, '1318': 260, '1320': 261, '1321': 262, '1322': 263, '1323': 264, '1324': 265, '1325': 266, '1326': 267, '1327': 268, '1328': 269, '1329': 270, '1330': 271, '1331': 272, '1332': 273, '1333': 274, '1334': 275, '1335': 276, '1336': 277, '1337': 278, '1339': 279, '1340': 280, '1341': 281, '1344': 282, '1345': 283, '1346': 284, '1347': 285, '1348': 286, '1349': 287, '1350': 288, '1351': 289, '1352': 290, '1353': 291, '1354': 292, '1355': 293, '1356': 294, '1357': 295, '1359': 296, '1361': 297, '1362': 298, '1363': 299, '1364': 300, '1365': 301, '1366': 302, '1367': 303, '1368': 304, '1372': 305, '1374': 306, '1376': 307, '1377': 308, '1379': 309, '1380': 310, '1381': 311, '1383': 312, '1384': 313, '1385': 314, '4009': 315, '4019': 316, '4022': 317, '4026': 318, '4030': 319, '4033': 320, '4035': 321, '4037': 322, '4041': 323, '4042': 324, '4044': 325, '4046': 326, '4051': 327, '4054': 328, '4058': 329, '4104': 330, '4105': 331, '4106': 332, '4107': 333, '4109': 334, '4111': 335, '4112': 336, '4115': 337, '4116': 338, '4117': 339, '4118': 340, '4119': 341, '4120': 342, '4122': 343, '4123': 344, '4125': 345, '4128': 346, '4130': 347, '4131': 348, '4133': 349, '4134': 350, '4135': 351, '4136': 352, '4140': 353, '4141': 354, '4142': 355, '4145': 356, '4146': 357, '4149': 358, '4150': 359, '4153': 360, '4155': 361, '4159': 362, '4162': 363, '4163': 364, '4164': 365, '4165': 366, '4166': 367, '4168': 368, '4170': 369, '4171': 370, '4172': 371, '4179': 372, '4181': 373, '4182': 374, '4184': 375, '4185': 376, '4188': 377, '4190': 378, '4191': 379, '4196': 380, '4200': 381, '4201': 382, '4205': 383, '4206': 384, '4210': 385, '4216': 386, '4219': 387, '4220': 388, '4221': 389, '4228': 390, '4230': 391, '4234': 392, '4235': 393, '4236': 394, '4237': 395, '4240': 396, '4241': 397, '4248': 398, '4255': 399, '4257': 400, '4263': 401, '4268': 402, '4273': 403, '4281': 404, '4282': 405, '4283': 406, '4284': 407, '4285': 408, '4286': 409, '4291': 410, '4298': 411, '4301': 412, '7025': 413, '7037': 414, '7038': 415, '7039': 416, '7040': 417, '7041': 418, '7043': 419, '7044': 420, '7045': 421, '7047': 422, '7048': 423, '7049': 424, '7051': 425, '7053': 426, '7056': 427, '7058': 428, '7059': 429, '7061': 430, '7062': 431, '7063': 432, '7064': 433, '7065': 434, '7066': 435, '7067': 436, '7068': 437, '7069': 438, '7070': 439, '7071': 440, '7072': 441, '7073': 442, '7074': 443, '7076': 444, '7077': 445, '7078': 446, '7079': 447, '7080': 448, '7081': 449, '7086': 450, '7087': 451, '7089': 452, '7090': 453, '7092': 454, '7093': 455, '7096': 456, '7097': 457, '7098': 458, '7099': 459, '7100': 460, '7102': 461, '7103': 462, '7104': 463, '7105': 464, '7106': 465, '7107': 466, '7108': 467, '7109': 468, '7110': 469, '7111': 470, '7112': 471, '7113': 472, '7115': 473, '7116': 474, '7117': 475, '7118': 476, '7119': 477, '7120': 478, '7122': 479, '7123': 480, '7124': 481, '7125': 482, '7126': 483, '7127': 484, '7128': 485, '7129': 486, '7130': 487, '7131': 488, '7132': 489, '7133': 490, '7134': 491, '7136': 492, '7137': 493, '7138': 494, '7139': 495, '7140': 496, '7141': 497, '7142': 498, '7143': 499, '7145': 500, '7146': 501, '7147': 502, '7148': 503, '7149': 504, '7150': 505, '7152': 506, '7154': 507, '7155': 508, '7156': 509, '7157': 510, '7158': 511, '7159': 512, '7160': 513, '7161': 514, '7162': 515, '7165': 516, '7166': 517, '7167': 518, '7168': 519, '7169': 520, '7170': 521, '7171': 522, '7172': 523, '7173': 524, '7174': 525, '7175': 526, '7176': 527, '7177': 528, '7178': 529, '7179': 530, '7180': 531, '7181': 532, '7182': 533, '7183': 534, '7184': 535, '7185': 536, '7186': 537, '7188': 538, '7189': 539, '7190': 540, '7192': 541, '7193': 542, '7194': 543, '7195': 544, '7196': 545, '7197': 546, '7198': 547, '7199': 548, '7200': 549, '7201': 550, '7202': 551, '7203': 552, '7204': 553, '7206': 554, '7207': 555, '7208': 556, '7209': 557, '7210': 558, '7211': 559, '7212': 560, '7213': 561, '7214': 562, '7215': 563, '7216': 564, '7217': 565, '7218': 566, '7219': 567, '7220': 568, '7221': 569, '7222': 570, '7223': 571, '7224': 572, '7225': 573, '7226': 574, '7227': 575, '7228': 576, '7229': 577, '7230': 578, '7231': 579, '7232': 580, '7233': 581, '7234': 582, '7235': 583, '7236': 584, '7237': 585, '7238': 586, '7239': 587, '7240': 588, '7241': 589, '7242': 590, '7243': 591, '7244': 592, '7245': 593, '7246': 594, '7247': 595, '7248': 596, '7249': 597, '7250': 598, '7251': 599, '7252': 600, '7253': 601, '7254': 602, '7255': 603, '7256': 604, '7257': 605, '7258': 606, '7259': 607, '7260': 608, '7261': 609, '7262': 610, '7263': 611, '7264': 612, '7265': 613, '7266': 614, '7267': 615, '7268': 616, '7269': 617, '7270': 618, '7271': 619, '7272': 620, '7273': 621, '7274': 622, '7275': 623, '7276': 624, '7277': 625, '7278': 626, '7279': 627, '7280': 628, '7282': 629, '7283': 630, '7284': 631, '7285': 632, '7286': 633, '7287': 634, '7288': 635, '7290': 636, '7291': 637, '7292': 638, '7293': 639, '7294': 640, '7295': 641, '7296': 642, '7297': 643, '7298': 644, '7299': 645, '7300': 646, '7301': 647, '7302': 648, '7304': 649, '7305': 650, '7306': 651, '7307': 652, '7308': 653, '7309': 654, '7310': 655, '7311': 656, '7312': 657, '7313': 658, '7314': 659, '7315': 660, '7316': 661, '7317': 662, '7318': 663, '7319': 664, '7320': 665, '7322': 666, '7323': 667, '7325': 668, '7326': 669, '7327': 670, '7328': 671, '7329': 672, '7330': 673, '7332': 674, '7334': 675, '7335': 676, '7336': 677, '7337': 678, '7338': 679, '7339': 680, '7340': 681, '7341': 682, '7343': 683, '7344': 684, '7345': 685, '7347': 686, '7348': 687, '7349': 688, '7350': 689, '7351': 690, '7352': 691, '7354': 692, '7355': 693, '7356': 694, '7357': 695, '7358': 696, '7360': 697, '7361': 698, '7362': 699, '7364': 700, '7365': 701, '7366': 702, '7367': 703, '7368': 704, '7369': 705, '7371': 706, '7372': 707, '7373': 708, '7374': 709, '7375': 710, '7376': 711, '7377': 712, '7378': 713, '7379': 714, '7381': 715, '7383': 716, '7384': 717, '7385': 718, '7386': 719, '7387': 720, '7388': 721, '7389': 722, '7390': 723, '7391': 724, '7392': 725, '7393': 726, '7394': 727, '7395': 728, '7396': 729, '7397': 730, '7398': 731, '7399': 732, '7401': 733, '7403': 734, '7404': 735, '7405': 736, '7406': 737, '7407': 738, '7409': 739, '7411': 740}, 'clinical_site': {'UAB': 0, 'UCSD': 1, 'UW': 2}, 'study_group': {'healthy': 0, 'insulin_dependent': 1, 'oral_medication_and_or_non_insulin_injectable_medication_controlled': 2, 'pre_diabetes_lifestyle_controlled': 3}, 'sleep_stage': {'awake': 0, 'deep': 1, 'light': 2, 'rem': 3, 'unknown': 4}}\n",
       "  \t\"embedding_paddings\":                []\n",
       "  \t\"embedding_sizes\":                   {'participant_id': (741, 65), 'clinical_site': (3, 3), 'study_group': (4, 3), 'sleep_stage': (5, 4)}\n",
       "  \t\"hidden_size\":                       32\n",
       "  \t\"initialization\":                    lecun_normal\n",
       "  \t\"interpolation_mode\":                linear\n",
       "  \t\"learning_rate\":                     0.0001\n",
       "  \t\"log_gradient_flow\":                 False\n",
       "  \t\"log_interval\":                      50\n",
       "  \t\"log_val_interval\":                  50\n",
       "  \t\"monotone_constraints\":              {}\n",
       "  \t\"n_blocks\":                          [1]\n",
       "  \t\"n_layers\":                          [1]\n",
       "  \t\"naive_level\":                       True\n",
       "  \t\"optimizer\":                         adam\n",
       "  \t\"optimizer_params\":                  None\n",
       "  \t\"output_size\":                       1\n",
       "  \t\"output_transformer\":                GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t)\n",
       "  \t\"pooling_mode\":                      max\n",
       "  \t\"pooling_sizes\":                     [2]\n",
       "  \t\"prediction_length\":                 12\n",
       "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "  \t\"reduce_on_plateau_patience\":        4\n",
       "  \t\"reduce_on_plateau_reduction\":       2.0\n",
       "  \t\"shared_weights\":                    True\n",
       "  \t\"static_categoricals\":               ['participant_id', 'clinical_site', 'study_group']\n",
       "  \t\"static_hidden_size\":                16\n",
       "  \t\"static_reals\":                      ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale']\n",
       "  \t\"time_varying_categoricals_decoder\": ['sleep_stage']\n",
       "  \t\"time_varying_categoricals_encoder\": ['sleep_stage']\n",
       "  \t\"time_varying_reals_decoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag']\n",
       "  \t\"time_varying_reals_encoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  \t\"weight_decay\":                      0.001\n",
       "  \t\"x_categoricals\":                    ['participant_id', 'clinical_site', 'study_group', 'sleep_stage']\n",
       "  \t\"x_reals\":                           ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale', 'ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  (loss): MAE()\n",
       "  (logging_metrics): ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "    (4): MASE()\n",
       "  )\n",
       "  (embeddings): MultiEmbedding(\n",
       "    (embeddings): ModuleDict(\n",
       "      (participant_id): Embedding(741, 65)\n",
       "      (clinical_site): Embedding(3, 3)\n",
       "      (study_group): Embedding(4, 3)\n",
       "      (sleep_stage): Embedding(5, 4)\n",
       "    )\n",
       "  )\n",
       "  (model): NHiTS(\n",
       "    (blocks): ModuleList(\n",
       "      (0): NHiTSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (static_encoder): StaticFeaturesEncoder(\n",
       "          (encoder): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Linear(in_features=75, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (layers): MLP(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=7540, out_features=32, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (4): ReLU()\n",
       "            (5): Dropout(p=0.1, inplace=False)\n",
       "            (6): Linear(in_features=32, out_features=294, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (basis): IdentityBasis()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NHITS older version\n",
    "\n",
    "# Load Data\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Fetch best ckpt from GCS (use your actual bucket/prefix)\n",
    "BUCKET = \"cgmproject2025\"\n",
    "GCS_PREFIX = \"checkpoints_nhits_288v4\"   # <- change if your run used a different folder\n",
    "best_ckpt_path = download_best_ckpt_by_filename(\"checkpoints\", BUCKET, GCS_PREFIX,\n",
    "                                                run_prefix=\"nhits_288-\", metric_key=\"val_loss\")\n",
    "\n",
    "#from TFT_pytorch import log_memory, create_tft_dataloaders, TFT_train\n",
    "from scripts.NHITS288v4 import create_nhits_dataloaders, NHiTS, load_nhits_from_gcs\n",
    "\n",
    "# Rebuild the training dataset (same context_length, horizon, etc.)\n",
    "training, val_dataloader, train_dataloader, validation = create_nhits_dataloaders(train, horizon=12, context_length=288, batchsize=32)\n",
    "\n",
    "# Load model from the checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = NHiTS.load_from_checkpoint(best_ckpt_path, map_location=device).to(device)\n",
    "model.eval()  # Put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d67264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in NHITS model: 301492\n"
     ]
    }
   ],
   "source": [
    "# Get the parameter count of tft\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in NHITS model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a76a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_preds: torch.Size([741, 12, 1])\n",
      "          Metric       Mean  Lower CI   Upper CI\n",
      "0          SMAPE   6.404618  6.030764   6.778471\n",
      "1            MAE   8.657538  8.060709   9.254367\n",
      "2           RMSE  10.155661  9.484127  10.827196\n",
      "3  Quantile_Loss   4.328769  4.030355   4.627183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15063/1184106390.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(\n"
     ]
    }
   ],
   "source": [
    "# Get global metrics:\n",
    "raw_preds = model.predict(val_dataloader, mode=\"raw\", return_x=True, return_index=True)\n",
    "print(\"Shape of raw_preds:\", raw_preds.output[\"prediction\"].shape)\n",
    "y_pred = raw_preds.output[\"prediction\"] #[:, :, 1] #To get median quantile.\n",
    "y_true = raw_preds.x[\"decoder_target\"]\n",
    "index_df = raw_preds.index\n",
    "records = []\n",
    "for i in range(len(index_df)):\n",
    "    uid = index_df.iloc[i][\"participant_id\"]\n",
    "    time_start = index_df.iloc[i][\"ds\"]\n",
    "    for t in range(y_pred.shape[1]):\n",
    "        records.append({\n",
    "            \"participant_id\": uid,\n",
    "            \"ds\": int(time_start + t),\n",
    "            \"target\": float(y_true[i, t]),\n",
    "            \"prediction\": float(y_pred[i, t]),\n",
    "        })\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "metricsCI = calculate_metrics_CI(records, train)\n",
    "print(metricsCI)\n",
    "\n",
    "# Save metrics locally\n",
    "metricsCI.to_csv(\"./figures/NHITS_288v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecffd6",
   "metadata": {},
   "source": [
    "## Best NHITS val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a90d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded best checkpoint: gs://cgmproject2025/checkpoints_nhits_288v10/nhits_288-epoch=06-val_loss=7.75.ckpt -> checkpoints/nhits_288-epoch=06-val_loss=7.75.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] NVIDIA A100-SXM4-40GB  CC=(8, 0), BF16=OK\n",
      "[2025-09-03 02:52:43.019302] ðŸš€ Start of Dataloader Creation\n",
      "GPU Mem allocated: 0.00 GB | reserved: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NHiTS(\n",
       "  \t\"activation\":                        ReLU\n",
       "  \t\"backcast_loss_ratio\":               0.2\n",
       "  \t\"batch_normalization\":               False\n",
       "  \t\"categorical_groups\":                {}\n",
       "  \t\"context_length\":                    288\n",
       "  \t\"dataset_parameters\":                {'time_idx': 'ds', 'target': 'cgm_glucose', 'group_ids': ['participant_id'], 'weight': None, 'max_encoder_length': 288, 'min_encoder_length': 288, 'min_prediction_idx': np.int64(11), 'min_prediction_length': 12, 'max_prediction_length': 12, 'static_categoricals': ['participant_id', 'clinical_site', 'study_group'], 'static_reals': ['age'], 'time_varying_known_categoricals': ['sleep_stage'], 'time_varying_known_reals': ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag'], 'time_varying_unknown_categoricals': None, 'time_varying_unknown_reals': ['cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std'], 'variable_groups': None, 'constant_fill_strategy': None, 'allow_missing_timesteps': False, 'lags': None, 'add_relative_time_idx': False, 'add_target_scales': True, 'add_encoder_length': True, 'target_normalizer': GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t), 'categorical_encoders': {'__group_id__participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'clinical_site': NaNLabelEncoder(add_nan=False, warn=True), 'study_group': NaNLabelEncoder(add_nan=False, warn=True), 'sleep_stage': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'age': StandardScaler(), 'encoder_length': StandardScaler(), 'cgm_glucose_center': StandardScaler(), 'cgm_glucose_scale': StandardScaler(), 'ds': StandardScaler(), 'minute_of_day': StandardScaler(), 'tod_sin': StandardScaler(), 'tod_cos': StandardScaler(), 'activity_steps': StandardScaler(), 'calories_value': StandardScaler(), 'heartrate': StandardScaler(), 'oxygen_saturation': StandardScaler(), 'respiration_rate': StandardScaler(), 'stress_level': StandardScaler(), 'predmeal_flag': StandardScaler(), 'cgm_lag_1': StandardScaler(), 'cgm_lag_3': StandardScaler(), 'cgm_lag_6': StandardScaler(), 'cgm_diff_lag_1': StandardScaler(), 'cgm_diff_lag_3': StandardScaler(), 'cgm_diff_lag_6': StandardScaler(), 'cgm_lagdiff_1_3': StandardScaler(), 'cgm_lagdiff_3_6': StandardScaler(), 'cgm_rolling_mean': StandardScaler(), 'cgm_rolling_std': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
       "  \t\"downsample_frequencies\":            [2, 2]\n",
       "  \t\"dropout\":                           0.1\n",
       "  \t\"embedding_labels\":                  {'participant_id': {'1023': 0, '1024': 1, '1026': 2, '1027': 3, '1028': 4, '1029': 5, '1030': 6, '1031': 7, '1032': 8, '1033': 9, '1034': 10, '1035': 11, '1036': 12, '1037': 13, '1038': 14, '1039': 15, '1040': 16, '1041': 17, '1042': 18, '1044': 19, '1045': 20, '1046': 21, '1047': 22, '1049': 23, '1050': 24, '1051': 25, '1052': 26, '1053': 27, '1054': 28, '1055': 29, '1056': 30, '1057': 31, '1058': 32, '1060': 33, '1061': 34, '1062': 35, '1063': 36, '1064': 37, '1065': 38, '1066': 39, '1067': 40, '1068': 41, '1069': 42, '1070': 43, '1071': 44, '1072': 45, '1073': 46, '1074': 47, '1075': 48, '1076': 49, '1077': 50, '1079': 51, '1080': 52, '1081': 53, '1083': 54, '1084': 55, '1085': 56, '1086': 57, '1087': 58, '1088': 59, '1089': 60, '1092': 61, '1093': 62, '1094': 63, '1095': 64, '1096': 65, '1097': 66, '1098': 67, '1099': 68, '1100': 69, '1101': 70, '1103': 71, '1104': 72, '1105': 73, '1106': 74, '1109': 75, '1110': 76, '1112': 77, '1114': 78, '1115': 79, '1116': 80, '1117': 81, '1118': 82, '1119': 83, '1120': 84, '1121': 85, '1122': 86, '1124': 87, '1125': 88, '1126': 89, '1128': 90, '1129': 91, '1131': 92, '1132': 93, '1133': 94, '1134': 95, '1135': 96, '1136': 97, '1137': 98, '1138': 99, '1139': 100, '1140': 101, '1141': 102, '1143': 103, '1144': 104, '1145': 105, '1146': 106, '1148': 107, '1149': 108, '1151': 109, '1152': 110, '1153': 111, '1154': 112, '1155': 113, '1156': 114, '1157': 115, '1158': 116, '1159': 117, '1160': 118, '1161': 119, '1163': 120, '1164': 121, '1166': 122, '1167': 123, '1168': 124, '1169': 125, '1170': 126, '1171': 127, '1172': 128, '1173': 129, '1174': 130, '1175': 131, '1176': 132, '1177': 133, '1178': 134, '1179': 135, '1180': 136, '1181': 137, '1183': 138, '1184': 139, '1185': 140, '1186': 141, '1188': 142, '1189': 143, '1193': 144, '1194': 145, '1195': 146, '1196': 147, '1198': 148, '1199': 149, '1200': 150, '1201': 151, '1202': 152, '1203': 153, '1204': 154, '1205': 155, '1206': 156, '1207': 157, '1208': 158, '1209': 159, '1210': 160, '1211': 161, '1212': 162, '1213': 163, '1214': 164, '1215': 165, '1216': 166, '1217': 167, '1218': 168, '1219': 169, '1220': 170, '1221': 171, '1222': 172, '1223': 173, '1224': 174, '1225': 175, '1226': 176, '1227': 177, '1228': 178, '1229': 179, '1230': 180, '1231': 181, '1232': 182, '1233': 183, '1234': 184, '1235': 185, '1236': 186, '1237': 187, '1238': 188, '1239': 189, '1241': 190, '1242': 191, '1243': 192, '1244': 193, '1245': 194, '1246': 195, '1247': 196, '1248': 197, '1249': 198, '1250': 199, '1251': 200, '1252': 201, '1253': 202, '1254': 203, '1255': 204, '1256': 205, '1257': 206, '1258': 207, '1259': 208, '1260': 209, '1261': 210, '1262': 211, '1263': 212, '1264': 213, '1266': 214, '1267': 215, '1268': 216, '1269': 217, '1270': 218, '1271': 219, '1272': 220, '1273': 221, '1274': 222, '1275': 223, '1276': 224, '1277': 225, '1278': 226, '1280': 227, '1281': 228, '1283': 229, '1284': 230, '1285': 231, '1286': 232, '1287': 233, '1289': 234, '1290': 235, '1291': 236, '1292': 237, '1293': 238, '1294': 239, '1295': 240, '1297': 241, '1298': 242, '1300': 243, '1302': 244, '1303': 245, '1304': 246, '1305': 247, '1306': 248, '1307': 249, '1308': 250, '1309': 251, '1310': 252, '1311': 253, '1312': 254, '1313': 255, '1314': 256, '1315': 257, '1316': 258, '1317': 259, '1318': 260, '1320': 261, '1321': 262, '1322': 263, '1323': 264, '1324': 265, '1325': 266, '1326': 267, '1327': 268, '1328': 269, '1329': 270, '1330': 271, '1331': 272, '1332': 273, '1333': 274, '1334': 275, '1335': 276, '1336': 277, '1337': 278, '1339': 279, '1340': 280, '1341': 281, '1344': 282, '1345': 283, '1346': 284, '1347': 285, '1348': 286, '1349': 287, '1350': 288, '1351': 289, '1352': 290, '1353': 291, '1354': 292, '1355': 293, '1356': 294, '1357': 295, '1359': 296, '1361': 297, '1362': 298, '1363': 299, '1364': 300, '1365': 301, '1366': 302, '1367': 303, '1368': 304, '1372': 305, '1374': 306, '1376': 307, '1377': 308, '1379': 309, '1380': 310, '1381': 311, '1383': 312, '1384': 313, '1385': 314, '4009': 315, '4019': 316, '4022': 317, '4026': 318, '4030': 319, '4033': 320, '4035': 321, '4037': 322, '4041': 323, '4042': 324, '4044': 325, '4046': 326, '4051': 327, '4054': 328, '4058': 329, '4104': 330, '4105': 331, '4106': 332, '4107': 333, '4109': 334, '4111': 335, '4112': 336, '4115': 337, '4116': 338, '4117': 339, '4118': 340, '4119': 341, '4120': 342, '4122': 343, '4123': 344, '4125': 345, '4128': 346, '4130': 347, '4131': 348, '4133': 349, '4134': 350, '4135': 351, '4136': 352, '4140': 353, '4141': 354, '4142': 355, '4145': 356, '4146': 357, '4149': 358, '4150': 359, '4153': 360, '4155': 361, '4159': 362, '4162': 363, '4163': 364, '4164': 365, '4165': 366, '4166': 367, '4168': 368, '4170': 369, '4171': 370, '4172': 371, '4179': 372, '4181': 373, '4182': 374, '4184': 375, '4185': 376, '4188': 377, '4190': 378, '4191': 379, '4196': 380, '4200': 381, '4201': 382, '4205': 383, '4206': 384, '4210': 385, '4216': 386, '4219': 387, '4220': 388, '4221': 389, '4228': 390, '4230': 391, '4234': 392, '4235': 393, '4236': 394, '4237': 395, '4240': 396, '4241': 397, '4248': 398, '4255': 399, '4257': 400, '4263': 401, '4268': 402, '4273': 403, '4281': 404, '4282': 405, '4283': 406, '4284': 407, '4285': 408, '4286': 409, '4291': 410, '4298': 411, '4301': 412, '7025': 413, '7037': 414, '7038': 415, '7039': 416, '7040': 417, '7041': 418, '7043': 419, '7044': 420, '7045': 421, '7047': 422, '7048': 423, '7049': 424, '7051': 425, '7053': 426, '7056': 427, '7058': 428, '7059': 429, '7061': 430, '7062': 431, '7063': 432, '7064': 433, '7065': 434, '7066': 435, '7067': 436, '7068': 437, '7069': 438, '7070': 439, '7071': 440, '7072': 441, '7073': 442, '7074': 443, '7076': 444, '7077': 445, '7078': 446, '7079': 447, '7080': 448, '7081': 449, '7086': 450, '7087': 451, '7089': 452, '7090': 453, '7092': 454, '7093': 455, '7096': 456, '7097': 457, '7098': 458, '7099': 459, '7100': 460, '7102': 461, '7103': 462, '7104': 463, '7105': 464, '7106': 465, '7107': 466, '7108': 467, '7109': 468, '7110': 469, '7111': 470, '7112': 471, '7113': 472, '7115': 473, '7116': 474, '7117': 475, '7118': 476, '7119': 477, '7120': 478, '7122': 479, '7123': 480, '7124': 481, '7125': 482, '7126': 483, '7127': 484, '7128': 485, '7129': 486, '7130': 487, '7131': 488, '7132': 489, '7133': 490, '7134': 491, '7136': 492, '7137': 493, '7138': 494, '7139': 495, '7140': 496, '7141': 497, '7142': 498, '7143': 499, '7145': 500, '7146': 501, '7147': 502, '7148': 503, '7149': 504, '7150': 505, '7152': 506, '7154': 507, '7155': 508, '7156': 509, '7157': 510, '7158': 511, '7159': 512, '7160': 513, '7161': 514, '7162': 515, '7165': 516, '7166': 517, '7167': 518, '7168': 519, '7169': 520, '7170': 521, '7171': 522, '7172': 523, '7173': 524, '7174': 525, '7175': 526, '7176': 527, '7177': 528, '7178': 529, '7179': 530, '7180': 531, '7181': 532, '7182': 533, '7183': 534, '7184': 535, '7185': 536, '7186': 537, '7188': 538, '7189': 539, '7190': 540, '7192': 541, '7193': 542, '7194': 543, '7195': 544, '7196': 545, '7197': 546, '7198': 547, '7199': 548, '7200': 549, '7201': 550, '7202': 551, '7203': 552, '7204': 553, '7206': 554, '7207': 555, '7208': 556, '7209': 557, '7210': 558, '7211': 559, '7212': 560, '7213': 561, '7214': 562, '7215': 563, '7216': 564, '7217': 565, '7218': 566, '7219': 567, '7220': 568, '7221': 569, '7222': 570, '7223': 571, '7224': 572, '7225': 573, '7226': 574, '7227': 575, '7228': 576, '7229': 577, '7230': 578, '7231': 579, '7232': 580, '7233': 581, '7234': 582, '7235': 583, '7236': 584, '7237': 585, '7238': 586, '7239': 587, '7240': 588, '7241': 589, '7242': 590, '7243': 591, '7244': 592, '7245': 593, '7246': 594, '7247': 595, '7248': 596, '7249': 597, '7250': 598, '7251': 599, '7252': 600, '7253': 601, '7254': 602, '7255': 603, '7256': 604, '7257': 605, '7258': 606, '7259': 607, '7260': 608, '7261': 609, '7262': 610, '7263': 611, '7264': 612, '7265': 613, '7266': 614, '7267': 615, '7268': 616, '7269': 617, '7270': 618, '7271': 619, '7272': 620, '7273': 621, '7274': 622, '7275': 623, '7276': 624, '7277': 625, '7278': 626, '7279': 627, '7280': 628, '7282': 629, '7283': 630, '7284': 631, '7285': 632, '7286': 633, '7287': 634, '7288': 635, '7290': 636, '7291': 637, '7292': 638, '7293': 639, '7294': 640, '7295': 641, '7296': 642, '7297': 643, '7298': 644, '7299': 645, '7300': 646, '7301': 647, '7302': 648, '7304': 649, '7305': 650, '7306': 651, '7307': 652, '7308': 653, '7309': 654, '7310': 655, '7311': 656, '7312': 657, '7313': 658, '7314': 659, '7315': 660, '7316': 661, '7317': 662, '7318': 663, '7319': 664, '7320': 665, '7322': 666, '7323': 667, '7325': 668, '7326': 669, '7327': 670, '7328': 671, '7329': 672, '7330': 673, '7332': 674, '7334': 675, '7335': 676, '7336': 677, '7337': 678, '7338': 679, '7339': 680, '7340': 681, '7341': 682, '7343': 683, '7344': 684, '7345': 685, '7347': 686, '7348': 687, '7349': 688, '7350': 689, '7351': 690, '7352': 691, '7354': 692, '7355': 693, '7356': 694, '7357': 695, '7358': 696, '7360': 697, '7361': 698, '7362': 699, '7364': 700, '7365': 701, '7366': 702, '7367': 703, '7368': 704, '7369': 705, '7371': 706, '7372': 707, '7373': 708, '7374': 709, '7375': 710, '7376': 711, '7377': 712, '7378': 713, '7379': 714, '7381': 715, '7383': 716, '7384': 717, '7385': 718, '7386': 719, '7387': 720, '7388': 721, '7389': 722, '7390': 723, '7391': 724, '7392': 725, '7393': 726, '7394': 727, '7395': 728, '7396': 729, '7397': 730, '7398': 731, '7399': 732, '7401': 733, '7403': 734, '7404': 735, '7405': 736, '7406': 737, '7407': 738, '7409': 739, '7411': 740}, 'clinical_site': {'UAB': 0, 'UCSD': 1, 'UW': 2}, 'study_group': {'healthy': 0, 'insulin_dependent': 1, 'oral_medication_and_or_non_insulin_injectable_medication_controlled': 2, 'pre_diabetes_lifestyle_controlled': 3}, 'sleep_stage': {'awake': 0, 'deep': 1, 'light': 2, 'rem': 3, 'unknown': 4}}\n",
       "  \t\"embedding_paddings\":                []\n",
       "  \t\"embedding_sizes\":                   {'participant_id': (741, 65), 'clinical_site': (3, 3), 'study_group': (4, 3), 'sleep_stage': (5, 4)}\n",
       "  \t\"hidden_size\":                       48\n",
       "  \t\"initialization\":                    lecun_normal\n",
       "  \t\"interpolation_mode\":                linear\n",
       "  \t\"learning_rate\":                     0.0001\n",
       "  \t\"log_gradient_flow\":                 False\n",
       "  \t\"log_interval\":                      50\n",
       "  \t\"log_val_interval\":                  50\n",
       "  \t\"monotone_constraints\":              {}\n",
       "  \t\"n_blocks\":                          [1, 1]\n",
       "  \t\"n_layers\":                          [1, 1]\n",
       "  \t\"naive_level\":                       True\n",
       "  \t\"optimizer\":                         adam\n",
       "  \t\"optimizer_params\":                  None\n",
       "  \t\"output_size\":                       1\n",
       "  \t\"output_transformer\":                GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t)\n",
       "  \t\"pooling_mode\":                      max\n",
       "  \t\"pooling_sizes\":                     [2, 2]\n",
       "  \t\"prediction_length\":                 12\n",
       "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "  \t\"reduce_on_plateau_patience\":        4\n",
       "  \t\"reduce_on_plateau_reduction\":       2.0\n",
       "  \t\"shared_weights\":                    True\n",
       "  \t\"static_categoricals\":               ['participant_id', 'clinical_site', 'study_group']\n",
       "  \t\"static_hidden_size\":                12\n",
       "  \t\"static_reals\":                      ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale']\n",
       "  \t\"time_varying_categoricals_decoder\": ['sleep_stage']\n",
       "  \t\"time_varying_categoricals_encoder\": ['sleep_stage']\n",
       "  \t\"time_varying_reals_decoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag']\n",
       "  \t\"time_varying_reals_encoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  \t\"weight_decay\":                      0.001\n",
       "  \t\"x_categoricals\":                    ['participant_id', 'clinical_site', 'study_group', 'sleep_stage']\n",
       "  \t\"x_reals\":                           ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale', 'ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  (loss): MAE()\n",
       "  (logging_metrics): ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "    (4): MASE()\n",
       "  )\n",
       "  (embeddings): MultiEmbedding(\n",
       "    (embeddings): ModuleDict(\n",
       "      (participant_id): Embedding(741, 65)\n",
       "      (clinical_site): Embedding(3, 3)\n",
       "      (study_group): Embedding(4, 3)\n",
       "      (sleep_stage): Embedding(5, 4)\n",
       "    )\n",
       "  )\n",
       "  (model): NHiTS(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x NHiTSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (static_encoder): StaticFeaturesEncoder(\n",
       "          (encoder): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Linear(in_features=75, out_features=12, bias=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (layers): MLP(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=7536, out_features=48, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (4): ReLU()\n",
       "            (5): Dropout(p=0.1, inplace=False)\n",
       "            (6): Linear(in_features=48, out_features=294, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (basis): IdentityBasis()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NHITS older version\n",
    "\n",
    "# Load Data\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Fetch best ckpt from GCS (use your actual bucket/prefix)\n",
    "BUCKET = \"cgmproject2025\"\n",
    "GCS_PREFIX = \"checkpoints_nhits_288v10\"   # <- change if your run used a different folder\n",
    "best_ckpt_path = download_best_ckpt_by_filename(\"checkpoints\", BUCKET, GCS_PREFIX,\n",
    "                                                run_prefix=\"nhits_288-\", metric_key=\"val_loss\")\n",
    "\n",
    "#from TFT_pytorch import log_memory, create_tft_dataloaders, TFT_train\n",
    "from scripts.NHITS288v10 import create_nhits_dataloaders, NHiTS, load_nhits_from_gcs\n",
    "\n",
    "# Rebuild the training dataset (same context_length, horizon, etc.)\n",
    "training, val_dataloader, train_dataloader, validation = create_nhits_dataloaders(train, horizon=12, context_length=288, batchsize=32)\n",
    "\n",
    "# Load model from the checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = NHiTS.load_from_checkpoint(best_ckpt_path, map_location=device).to(device)\n",
    "model.eval()  # Put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab929057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in NHITS model: 807098\n"
     ]
    }
   ],
   "source": [
    "# Get the parameter count of tft\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in NHITS model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fadec935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_preds: torch.Size([741, 12, 1])\n",
      "          Metric      Mean  Lower CI   Upper CI\n",
      "0          SMAPE  6.311799  5.930420   6.693177\n",
      "1            MAE  8.466096  7.914444   9.017749\n",
      "2           RMSE  9.933878  9.313917  10.553839\n",
      "3  Quantile_Loss  4.233048  3.957222   4.508874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2217/1184106390.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(\n"
     ]
    }
   ],
   "source": [
    "# Get global metrics:\n",
    "raw_preds = model.predict(val_dataloader, mode=\"raw\", return_x=True, return_index=True)\n",
    "print(\"Shape of raw_preds:\", raw_preds.output[\"prediction\"].shape)\n",
    "y_pred = raw_preds.output[\"prediction\"] #[:, :, 1] #To get median quantile.\n",
    "y_true = raw_preds.x[\"decoder_target\"]\n",
    "index_df = raw_preds.index\n",
    "records = []\n",
    "for i in range(len(index_df)):\n",
    "    uid = index_df.iloc[i][\"participant_id\"]\n",
    "    time_start = index_df.iloc[i][\"ds\"]\n",
    "    for t in range(y_pred.shape[1]):\n",
    "        records.append({\n",
    "            \"participant_id\": uid,\n",
    "            \"ds\": int(time_start + t),\n",
    "            \"target\": float(y_true[i, t]),\n",
    "            \"prediction\": float(y_pred[i, t]),\n",
    "        })\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "metricsCI = calculate_metrics_CI(records, train)\n",
    "print(metricsCI)\n",
    "\n",
    "# Save metrics locally\n",
    "metricsCI.to_csv(\"./figures/NHITS_288best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6618e66",
   "metadata": {},
   "source": [
    "# NHITS v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8211cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded best checkpoint: gs://cgmproject2025/checkpoints_nhits_288v2/nhits_288-epoch=01-val_loss=8.53.ckpt -> checkpoints/nhits_288-epoch=01-val_loss=8.53.ckpt\n",
      "[2025-09-02 03:29:25.459498] ðŸš€ Start of Dataloader Creation\n",
      "GPU Mem allocated: 0.00 GB | reserved: 0.03 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/shaksonisaac/miniconda3/envs/cgmall/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NHiTS(\n",
       "  \t\"activation\":                        ReLU\n",
       "  \t\"backcast_loss_ratio\":               0.0\n",
       "  \t\"batch_normalization\":               False\n",
       "  \t\"categorical_groups\":                {}\n",
       "  \t\"context_length\":                    288\n",
       "  \t\"dataset_parameters\":                {'time_idx': 'ds', 'target': 'cgm_glucose', 'group_ids': ['participant_id'], 'weight': None, 'max_encoder_length': 288, 'min_encoder_length': 288, 'min_prediction_idx': np.int64(11), 'min_prediction_length': 12, 'max_prediction_length': 12, 'static_categoricals': ['participant_id', 'clinical_site', 'study_group'], 'static_reals': ['age'], 'time_varying_known_categoricals': ['sleep_stage'], 'time_varying_known_reals': ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag'], 'time_varying_unknown_categoricals': None, 'time_varying_unknown_reals': ['cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std'], 'variable_groups': None, 'constant_fill_strategy': None, 'allow_missing_timesteps': False, 'lags': None, 'add_relative_time_idx': False, 'add_target_scales': True, 'add_encoder_length': True, 'target_normalizer': GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t), 'categorical_encoders': {'__group_id__participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'participant_id': NaNLabelEncoder(add_nan=False, warn=True), 'clinical_site': NaNLabelEncoder(add_nan=False, warn=True), 'study_group': NaNLabelEncoder(add_nan=False, warn=True), 'sleep_stage': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'age': StandardScaler(), 'encoder_length': StandardScaler(), 'cgm_glucose_center': StandardScaler(), 'cgm_glucose_scale': StandardScaler(), 'ds': StandardScaler(), 'minute_of_day': StandardScaler(), 'tod_sin': StandardScaler(), 'tod_cos': StandardScaler(), 'activity_steps': StandardScaler(), 'calories_value': StandardScaler(), 'heartrate': StandardScaler(), 'oxygen_saturation': StandardScaler(), 'respiration_rate': StandardScaler(), 'stress_level': StandardScaler(), 'predmeal_flag': StandardScaler(), 'cgm_lag_1': StandardScaler(), 'cgm_lag_3': StandardScaler(), 'cgm_lag_6': StandardScaler(), 'cgm_diff_lag_1': StandardScaler(), 'cgm_diff_lag_3': StandardScaler(), 'cgm_diff_lag_6': StandardScaler(), 'cgm_lagdiff_1_3': StandardScaler(), 'cgm_lagdiff_3_6': StandardScaler(), 'cgm_rolling_mean': StandardScaler(), 'cgm_rolling_std': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
       "  \t\"downsample_frequencies\":            [2, 2]\n",
       "  \t\"dropout\":                           0.1\n",
       "  \t\"embedding_labels\":                  {'participant_id': {'1023': 0, '1024': 1, '1026': 2, '1027': 3, '1028': 4, '1029': 5, '1030': 6, '1031': 7, '1032': 8, '1033': 9, '1034': 10, '1035': 11, '1036': 12, '1037': 13, '1038': 14, '1039': 15, '1040': 16, '1041': 17, '1042': 18, '1044': 19, '1045': 20, '1046': 21, '1047': 22, '1049': 23, '1050': 24, '1051': 25, '1052': 26, '1053': 27, '1054': 28, '1055': 29, '1056': 30, '1057': 31, '1058': 32, '1060': 33, '1061': 34, '1062': 35, '1063': 36, '1064': 37, '1065': 38, '1066': 39, '1067': 40, '1068': 41, '1069': 42, '1070': 43, '1071': 44, '1072': 45, '1073': 46, '1074': 47, '1075': 48, '1076': 49, '1077': 50, '1079': 51, '1080': 52, '1081': 53, '1083': 54, '1084': 55, '1085': 56, '1086': 57, '1087': 58, '1088': 59, '1089': 60, '1092': 61, '1093': 62, '1094': 63, '1095': 64, '1096': 65, '1097': 66, '1098': 67, '1099': 68, '1100': 69, '1101': 70, '1103': 71, '1104': 72, '1105': 73, '1106': 74, '1109': 75, '1110': 76, '1112': 77, '1114': 78, '1115': 79, '1116': 80, '1117': 81, '1118': 82, '1119': 83, '1120': 84, '1121': 85, '1122': 86, '1124': 87, '1125': 88, '1126': 89, '1128': 90, '1129': 91, '1131': 92, '1132': 93, '1133': 94, '1134': 95, '1135': 96, '1136': 97, '1137': 98, '1138': 99, '1139': 100, '1140': 101, '1141': 102, '1143': 103, '1144': 104, '1145': 105, '1146': 106, '1148': 107, '1149': 108, '1151': 109, '1152': 110, '1153': 111, '1154': 112, '1155': 113, '1156': 114, '1157': 115, '1158': 116, '1159': 117, '1160': 118, '1161': 119, '1163': 120, '1164': 121, '1166': 122, '1167': 123, '1168': 124, '1169': 125, '1170': 126, '1171': 127, '1172': 128, '1173': 129, '1174': 130, '1175': 131, '1176': 132, '1177': 133, '1178': 134, '1179': 135, '1180': 136, '1181': 137, '1183': 138, '1184': 139, '1185': 140, '1186': 141, '1188': 142, '1189': 143, '1193': 144, '1194': 145, '1195': 146, '1196': 147, '1198': 148, '1199': 149, '1200': 150, '1201': 151, '1202': 152, '1203': 153, '1204': 154, '1205': 155, '1206': 156, '1207': 157, '1208': 158, '1209': 159, '1210': 160, '1211': 161, '1212': 162, '1213': 163, '1214': 164, '1215': 165, '1216': 166, '1217': 167, '1218': 168, '1219': 169, '1220': 170, '1221': 171, '1222': 172, '1223': 173, '1224': 174, '1225': 175, '1226': 176, '1227': 177, '1228': 178, '1229': 179, '1230': 180, '1231': 181, '1232': 182, '1233': 183, '1234': 184, '1235': 185, '1236': 186, '1237': 187, '1238': 188, '1239': 189, '1241': 190, '1242': 191, '1243': 192, '1244': 193, '1245': 194, '1246': 195, '1247': 196, '1248': 197, '1249': 198, '1250': 199, '1251': 200, '1252': 201, '1253': 202, '1254': 203, '1255': 204, '1256': 205, '1257': 206, '1258': 207, '1259': 208, '1260': 209, '1261': 210, '1262': 211, '1263': 212, '1264': 213, '1266': 214, '1267': 215, '1268': 216, '1269': 217, '1270': 218, '1271': 219, '1272': 220, '1273': 221, '1274': 222, '1275': 223, '1276': 224, '1277': 225, '1278': 226, '1280': 227, '1281': 228, '1283': 229, '1284': 230, '1285': 231, '1286': 232, '1287': 233, '1289': 234, '1290': 235, '1291': 236, '1292': 237, '1293': 238, '1294': 239, '1295': 240, '1297': 241, '1298': 242, '1300': 243, '1302': 244, '1303': 245, '1304': 246, '1305': 247, '1306': 248, '1307': 249, '1308': 250, '1309': 251, '1310': 252, '1311': 253, '1312': 254, '1313': 255, '1314': 256, '1315': 257, '1316': 258, '1317': 259, '1318': 260, '1320': 261, '1321': 262, '1322': 263, '1323': 264, '1324': 265, '1325': 266, '1326': 267, '1327': 268, '1328': 269, '1329': 270, '1330': 271, '1331': 272, '1332': 273, '1333': 274, '1334': 275, '1335': 276, '1336': 277, '1337': 278, '1339': 279, '1340': 280, '1341': 281, '1344': 282, '1345': 283, '1346': 284, '1347': 285, '1348': 286, '1349': 287, '1350': 288, '1351': 289, '1352': 290, '1353': 291, '1354': 292, '1355': 293, '1356': 294, '1357': 295, '1359': 296, '1361': 297, '1362': 298, '1363': 299, '1364': 300, '1365': 301, '1366': 302, '1367': 303, '1368': 304, '1372': 305, '1374': 306, '1376': 307, '1377': 308, '1379': 309, '1380': 310, '1381': 311, '1383': 312, '1384': 313, '1385': 314, '4009': 315, '4019': 316, '4022': 317, '4026': 318, '4030': 319, '4033': 320, '4035': 321, '4037': 322, '4041': 323, '4042': 324, '4044': 325, '4046': 326, '4051': 327, '4054': 328, '4058': 329, '4104': 330, '4105': 331, '4106': 332, '4107': 333, '4109': 334, '4111': 335, '4112': 336, '4115': 337, '4116': 338, '4117': 339, '4118': 340, '4119': 341, '4120': 342, '4122': 343, '4123': 344, '4125': 345, '4128': 346, '4130': 347, '4131': 348, '4133': 349, '4134': 350, '4135': 351, '4136': 352, '4140': 353, '4141': 354, '4142': 355, '4145': 356, '4146': 357, '4149': 358, '4150': 359, '4153': 360, '4155': 361, '4159': 362, '4162': 363, '4163': 364, '4164': 365, '4165': 366, '4166': 367, '4168': 368, '4170': 369, '4171': 370, '4172': 371, '4179': 372, '4181': 373, '4182': 374, '4184': 375, '4185': 376, '4188': 377, '4190': 378, '4191': 379, '4196': 380, '4200': 381, '4201': 382, '4205': 383, '4206': 384, '4210': 385, '4216': 386, '4219': 387, '4220': 388, '4221': 389, '4228': 390, '4230': 391, '4234': 392, '4235': 393, '4236': 394, '4237': 395, '4240': 396, '4241': 397, '4248': 398, '4255': 399, '4257': 400, '4263': 401, '4268': 402, '4273': 403, '4281': 404, '4282': 405, '4283': 406, '4284': 407, '4285': 408, '4286': 409, '4291': 410, '4298': 411, '4301': 412, '7025': 413, '7037': 414, '7038': 415, '7039': 416, '7040': 417, '7041': 418, '7043': 419, '7044': 420, '7045': 421, '7047': 422, '7048': 423, '7049': 424, '7051': 425, '7053': 426, '7056': 427, '7058': 428, '7059': 429, '7061': 430, '7062': 431, '7063': 432, '7064': 433, '7065': 434, '7066': 435, '7067': 436, '7068': 437, '7069': 438, '7070': 439, '7071': 440, '7072': 441, '7073': 442, '7074': 443, '7076': 444, '7077': 445, '7078': 446, '7079': 447, '7080': 448, '7081': 449, '7086': 450, '7087': 451, '7089': 452, '7090': 453, '7092': 454, '7093': 455, '7096': 456, '7097': 457, '7098': 458, '7099': 459, '7100': 460, '7102': 461, '7103': 462, '7104': 463, '7105': 464, '7106': 465, '7107': 466, '7108': 467, '7109': 468, '7110': 469, '7111': 470, '7112': 471, '7113': 472, '7115': 473, '7116': 474, '7117': 475, '7118': 476, '7119': 477, '7120': 478, '7122': 479, '7123': 480, '7124': 481, '7125': 482, '7126': 483, '7127': 484, '7128': 485, '7129': 486, '7130': 487, '7131': 488, '7132': 489, '7133': 490, '7134': 491, '7136': 492, '7137': 493, '7138': 494, '7139': 495, '7140': 496, '7141': 497, '7142': 498, '7143': 499, '7145': 500, '7146': 501, '7147': 502, '7148': 503, '7149': 504, '7150': 505, '7152': 506, '7154': 507, '7155': 508, '7156': 509, '7157': 510, '7158': 511, '7159': 512, '7160': 513, '7161': 514, '7162': 515, '7165': 516, '7166': 517, '7167': 518, '7168': 519, '7169': 520, '7170': 521, '7171': 522, '7172': 523, '7173': 524, '7174': 525, '7175': 526, '7176': 527, '7177': 528, '7178': 529, '7179': 530, '7180': 531, '7181': 532, '7182': 533, '7183': 534, '7184': 535, '7185': 536, '7186': 537, '7188': 538, '7189': 539, '7190': 540, '7192': 541, '7193': 542, '7194': 543, '7195': 544, '7196': 545, '7197': 546, '7198': 547, '7199': 548, '7200': 549, '7201': 550, '7202': 551, '7203': 552, '7204': 553, '7206': 554, '7207': 555, '7208': 556, '7209': 557, '7210': 558, '7211': 559, '7212': 560, '7213': 561, '7214': 562, '7215': 563, '7216': 564, '7217': 565, '7218': 566, '7219': 567, '7220': 568, '7221': 569, '7222': 570, '7223': 571, '7224': 572, '7225': 573, '7226': 574, '7227': 575, '7228': 576, '7229': 577, '7230': 578, '7231': 579, '7232': 580, '7233': 581, '7234': 582, '7235': 583, '7236': 584, '7237': 585, '7238': 586, '7239': 587, '7240': 588, '7241': 589, '7242': 590, '7243': 591, '7244': 592, '7245': 593, '7246': 594, '7247': 595, '7248': 596, '7249': 597, '7250': 598, '7251': 599, '7252': 600, '7253': 601, '7254': 602, '7255': 603, '7256': 604, '7257': 605, '7258': 606, '7259': 607, '7260': 608, '7261': 609, '7262': 610, '7263': 611, '7264': 612, '7265': 613, '7266': 614, '7267': 615, '7268': 616, '7269': 617, '7270': 618, '7271': 619, '7272': 620, '7273': 621, '7274': 622, '7275': 623, '7276': 624, '7277': 625, '7278': 626, '7279': 627, '7280': 628, '7282': 629, '7283': 630, '7284': 631, '7285': 632, '7286': 633, '7287': 634, '7288': 635, '7290': 636, '7291': 637, '7292': 638, '7293': 639, '7294': 640, '7295': 641, '7296': 642, '7297': 643, '7298': 644, '7299': 645, '7300': 646, '7301': 647, '7302': 648, '7304': 649, '7305': 650, '7306': 651, '7307': 652, '7308': 653, '7309': 654, '7310': 655, '7311': 656, '7312': 657, '7313': 658, '7314': 659, '7315': 660, '7316': 661, '7317': 662, '7318': 663, '7319': 664, '7320': 665, '7322': 666, '7323': 667, '7325': 668, '7326': 669, '7327': 670, '7328': 671, '7329': 672, '7330': 673, '7332': 674, '7334': 675, '7335': 676, '7336': 677, '7337': 678, '7338': 679, '7339': 680, '7340': 681, '7341': 682, '7343': 683, '7344': 684, '7345': 685, '7347': 686, '7348': 687, '7349': 688, '7350': 689, '7351': 690, '7352': 691, '7354': 692, '7355': 693, '7356': 694, '7357': 695, '7358': 696, '7360': 697, '7361': 698, '7362': 699, '7364': 700, '7365': 701, '7366': 702, '7367': 703, '7368': 704, '7369': 705, '7371': 706, '7372': 707, '7373': 708, '7374': 709, '7375': 710, '7376': 711, '7377': 712, '7378': 713, '7379': 714, '7381': 715, '7383': 716, '7384': 717, '7385': 718, '7386': 719, '7387': 720, '7388': 721, '7389': 722, '7390': 723, '7391': 724, '7392': 725, '7393': 726, '7394': 727, '7395': 728, '7396': 729, '7397': 730, '7398': 731, '7399': 732, '7401': 733, '7403': 734, '7404': 735, '7405': 736, '7406': 737, '7407': 738, '7409': 739, '7411': 740}, 'clinical_site': {'UAB': 0, 'UCSD': 1, 'UW': 2}, 'study_group': {'healthy': 0, 'insulin_dependent': 1, 'oral_medication_and_or_non_insulin_injectable_medication_controlled': 2, 'pre_diabetes_lifestyle_controlled': 3}, 'sleep_stage': {'awake': 0, 'deep': 1, 'light': 2, 'rem': 3, 'unknown': 4}}\n",
       "  \t\"embedding_paddings\":                []\n",
       "  \t\"embedding_sizes\":                   {'participant_id': (741, 65), 'clinical_site': (3, 3), 'study_group': (4, 3), 'sleep_stage': (5, 4)}\n",
       "  \t\"hidden_size\":                       64\n",
       "  \t\"initialization\":                    lecun_normal\n",
       "  \t\"interpolation_mode\":                linear\n",
       "  \t\"learning_rate\":                     0.001\n",
       "  \t\"log_gradient_flow\":                 False\n",
       "  \t\"log_interval\":                      50\n",
       "  \t\"log_val_interval\":                  50\n",
       "  \t\"monotone_constraints\":              {}\n",
       "  \t\"n_blocks\":                          [1, 1]\n",
       "  \t\"n_layers\":                          [1, 1]\n",
       "  \t\"naive_level\":                       True\n",
       "  \t\"optimizer\":                         adam\n",
       "  \t\"optimizer_params\":                  None\n",
       "  \t\"output_size\":                       1\n",
       "  \t\"output_transformer\":                GroupNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tgroups=['participant_id'],\n",
       "  \t\tcenter=True,\n",
       "  \t\tscale_by_group=False,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t)\n",
       "  \t\"pooling_mode\":                      max\n",
       "  \t\"pooling_sizes\":                     [2, 2]\n",
       "  \t\"prediction_length\":                 12\n",
       "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "  \t\"reduce_on_plateau_patience\":        4\n",
       "  \t\"reduce_on_plateau_reduction\":       2.0\n",
       "  \t\"shared_weights\":                    True\n",
       "  \t\"static_categoricals\":               ['participant_id', 'clinical_site', 'study_group']\n",
       "  \t\"static_hidden_size\":                32\n",
       "  \t\"static_reals\":                      ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale']\n",
       "  \t\"time_varying_categoricals_decoder\": ['sleep_stage']\n",
       "  \t\"time_varying_categoricals_encoder\": ['sleep_stage']\n",
       "  \t\"time_varying_reals_decoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag']\n",
       "  \t\"time_varying_reals_encoder\":        ['ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  \t\"weight_decay\":                      0.001\n",
       "  \t\"x_categoricals\":                    ['participant_id', 'clinical_site', 'study_group', 'sleep_stage']\n",
       "  \t\"x_reals\":                           ['age', 'encoder_length', 'cgm_glucose_center', 'cgm_glucose_scale', 'ds', 'minute_of_day', 'tod_sin', 'tod_cos', 'activity_steps', 'calories_value', 'heartrate', 'oxygen_saturation', 'respiration_rate', 'stress_level', 'predmeal_flag', 'cgm_glucose', 'cgm_lag_1', 'cgm_lag_3', 'cgm_lag_6', 'cgm_diff_lag_1', 'cgm_diff_lag_3', 'cgm_diff_lag_6', 'cgm_lagdiff_1_3', 'cgm_lagdiff_3_6', 'cgm_rolling_mean', 'cgm_rolling_std']\n",
       "  (loss): MAE()\n",
       "  (logging_metrics): ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "    (4): MASE()\n",
       "  )\n",
       "  (embeddings): MultiEmbedding(\n",
       "    (embeddings): ModuleDict(\n",
       "      (participant_id): Embedding(741, 65)\n",
       "      (clinical_site): Embedding(3, 3)\n",
       "      (study_group): Embedding(4, 3)\n",
       "      (sleep_stage): Embedding(5, 4)\n",
       "    )\n",
       "  )\n",
       "  (model): NHiTS(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x NHiTSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (static_encoder): StaticFeaturesEncoder(\n",
       "          (encoder): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Linear(in_features=75, out_features=32, bias=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (layers): MLP(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=7556, out_features=64, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (4): ReLU()\n",
       "            (5): Dropout(p=0.1, inplace=False)\n",
       "            (6): Linear(in_features=64, out_features=294, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (basis): IdentityBasis()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NHITS older version\n",
    "\n",
    "# Load Data\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Fetch best ckpt from GCS (use your actual bucket/prefix)\n",
    "BUCKET = \"cgmproject2025\"\n",
    "GCS_PREFIX = \"checkpoints_nhits_288v2\"   # <- change if your run used a different folder\n",
    "best_ckpt_path = download_best_ckpt_by_filename(\"checkpoints\", BUCKET, GCS_PREFIX,\n",
    "                                                run_prefix=\"nhits_288-\", metric_key=\"val_loss\")\n",
    "\n",
    "#from TFT_pytorch import log_memory, create_tft_dataloaders, TFT_train\n",
    "from scripts.NHITS288v2 import create_nhits_dataloaders, NHiTS, load_nhits_from_gcs\n",
    "\n",
    "# Rebuild the training dataset (same context_length, horizon, etc.)\n",
    "training, val_dataloader, train_dataloader, validation = create_nhits_dataloaders(train, horizon=12, context_length=288, batchsize=32)\n",
    "\n",
    "# Load model from the checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = NHiTS.load_from_checkpoint(best_ckpt_path, map_location=device).to(device)\n",
    "model.eval()  # Put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "205bd781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_preds: torch.Size([741, 12, 1])\n",
      "          Metric       Mean   Lower CI   Upper CI\n",
      "0          SMAPE   6.769904   6.368118   7.171690\n",
      "1            MAE   9.193850   8.573059   9.814642\n",
      "2           RMSE  10.776200  10.082285  11.470115\n",
      "3  Quantile_Loss   4.596925   4.286529   4.907321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2393/1184106390.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(\n"
     ]
    }
   ],
   "source": [
    "# Get global metrics:\n",
    "raw_preds = model.predict(val_dataloader, mode=\"raw\", return_x=True, return_index=True)\n",
    "print(\"Shape of raw_preds:\", raw_preds.output[\"prediction\"].shape)\n",
    "y_pred = raw_preds.output[\"prediction\"]#[:, :, 1] #To get median quantile.\n",
    "y_true = raw_preds.x[\"decoder_target\"]\n",
    "index_df = raw_preds.index\n",
    "records = []\n",
    "for i in range(len(index_df)):\n",
    "    uid = index_df.iloc[i][\"participant_id\"]\n",
    "    time_start = index_df.iloc[i][\"ds\"]\n",
    "    for t in range(y_pred.shape[1]):\n",
    "        records.append({\n",
    "            \"participant_id\": uid,\n",
    "            \"ds\": int(time_start + t),\n",
    "            \"target\": float(y_true[i, t]),\n",
    "            \"prediction\": float(y_pred[i, t]),\n",
    "        })\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "metricsCI = calculate_metrics_CI(records, train)\n",
    "print(metricsCI)\n",
    "\n",
    "# Save metrics locally\n",
    "metricsCI.to_csv(\"./figures/NHITS_288v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgmall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
